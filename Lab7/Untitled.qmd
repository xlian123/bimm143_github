---
title: "Lab 7"
author: "Xiaoyu Lian (A17025943)"
format: pdf
editor: visual
---

Today we are going to learn how to apply different machine learning methods, beginning with clustering:

The goal here is to find groups/ clusters in your input data

first I will make up some data with clear groups. For this I will use the `rnorm()` function:
```{r}
rnorm(10)
hist(rnorm(10000, mean = 3))
```

```{r}
a <- hist(c(rnorm(10000,3),
       rnorm(10000,-3)))
a
```

　
```{r}
n <- 30
x <- c(rnorm(n,3), rnorm (n, -3))
y<- rev(x)
z<- cbind(x,y)
head(z)
hist(z)
```
Use the `kmeans()` function setting k to 2 and nstart = 20
inspect the results
>Q. How many points are in each cluster
>Q. What 'component'of your result object details
  - cluster size
  - cluster assignment
  - cluster mist

plot x colored by the kmeans cluster assignment and add cluster centers as blue points

```{r}
km <- kmeans(z, centers = 2)
km

```
Results in kmeans object `km`
```{r}
attributes(km)
```
cluster size
```{r}
km$totss
```
number of points in each cluster
```{r}
km$size
```
cluster assignment / membership
```{r}
km$cluster
```

>Q. plot z colored by the kmeans cluster assignment and add cluster centers as blue points

It will recluse the shorter color vector to be the same length as the larger (numbers of data point in z)

```{r}
plot(z, col = c("red","blue"))
```
```{r}
plot(z, col = km$cluster)
```
We can use the `points()` function to add new points to an existing plot...like the cluster centers
　
```{r}
plot(z, col = km$cluster)
points(km$centers, col = "blue", pch = 15, cex = 3)
```
>Q. Can you run kmeans and ask for 4 clusters please and plot the results like we have done above?

```{r}
km2 <- kmeans(z, centers = 4)
plot(z, col = km2$cluster)
points(km2$centers, col = "blue", pch = 15)
```
## Hierarchical Clustering

Let's take our some made-up data `z` and see how cluster work

first we need a distance matrix of our data to be clustered
```{r}
d <- dist(z)
hc <- hclust(d)
hc
```

```{r}
plot(hc)
abline(h = 3, col = "red")
```
I can get my cluster membership vector by "cutting the tree" with the `cutree()` function
```{r}
grps <- cutree(hc, h = 8)
```
Can you plot `z` colored by our hclust results:
```{r}
plot(z, col = grps)
```
>Q1. How many rows and columns are in your new data frame named x? What R functions could you use to answer this questions?
>Q2. Which approach to solving the ‘row-names problem’ mentioned above do you prefer and why? Is one approach more robust than another under certain circumstances?

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url, row.names = 1)
```

```{r}
dim(x)
```
```{r}
head(x)
```
```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```
>Q3: Changing what optional argument in the above barplot() function results in the following plot?

```{r}
barplot(as.matrix(x), beside=F, col=rainbow(nrow(x)))
```
A so-called "Pairs" plot can be useful for small datasets like this one
>Q5: Generating all pairwise plots may help somewhat. Can you make sense of the following code and resulting figure? What does it mean if a given point lies on the diagonal for a given plot?

```{r}
pairs(x, col=rainbow(10), pch=16)
#Country name is the x-axis for all the graphs in that colomn
```
It is hard to see structure and trends in even this small data set. How will we ever do this when we have big data set with 1000s or 10s of thousands of things we are measuring...

###PCA to Rescue

Lets see how PCA deals with this dataset. So main function in base R to do PCA is called `prcomp()`
```{r}
pca <- prcomp(t(x))
summary(pca)
```
Let's see what is inside this `pca` object that we created from running `prcomp()`
```{r}
attributes(pca)
```
```{r}
pca$x
```
```{r}
plot(pca$x[,1], pca$x[,2], col = c("black", "red", "blue", "green"), pch = 16,
     xlab = "PC1", ylab = "PC2")
```

```{r}
plot(pca$x[,1], pca$x[,2], col = c("black", "red", "blue", "green"),
     xlab = "PC1", ylab = "PC2")
text(pca$x[,1], pca$x[,2], colnames(x), col = c("black", "red", "blue", "green"))
```
```{r}
v <- round( pca$sdev^2/sum(pca$sdev^2) * 100 )
v
```

```{r}
z <- summary(pca)
z$importance
```

```{r}
barplot(v, xlab="Principal Component", ylab="Percent Variation")
```

```{r}
## Lets focus on PC1 as it accounts for > 90% of variance 
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,1], las=2 )
```



